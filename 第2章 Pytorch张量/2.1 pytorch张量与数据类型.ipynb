{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0b2cd018fd8cba4dc5007a84c518ef0aff2e9cf01ffb8111223e88fed299c237b",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.8.1+cu101\nTrue\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.1653, 0.3245, 0.5600],\n",
       "        [0.7287, 0.5149, 0.8560]])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# 生成固定shape的均匀分布的随机数\n",
    "x = torch.rand(size=(2, 3))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 0.2460,  0.9664,  1.7152],\n",
       "        [-0.6973, -0.3124,  0.5289]])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "x = torch.randn(size=(2, 3))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "x = torch.zeros(size=(2, 3))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "x = torch.ones(size=(2, 3, 4), dtype=torch.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# x.size()与shape的实现效果相同，好处在于size()方法能够对尺寸数组中的值进行切片\n",
    "x.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据类型：\n",
    "\"\"\"\n",
    "32位浮点型：torch.float32\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([6., 2.])"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "x = torch.tensor(data=[6, 2], dtype=torch.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([6., 2.])"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "# 将float32类型的数据转化为其他数据类型\n",
    "x.type(dtype=torch.int64)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-0.55233842, -0.80747907, -1.13826822],\n",
       "       [ 0.3642144 ,  0.90537777,  0.51207917]])"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "# tensor与numpy数据类型的转换\n",
    "x = np.random.randn(2, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.5523, -0.8075, -1.1383],\n",
       "        [ 0.3642,  0.9054,  0.5121]], dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "x = torch.from_numpy(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-0.55233842, -0.80747907, -1.13826822],\n",
       "       [ 0.3642144 ,  0.90537777,  0.51207917]])"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "source": [
    "## 张量的计算"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.9407, 1.1364, 1.2806],\n",
       "        [0.4761, 1.5148, 0.8901]])"
      ]
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "x1 = torch.rand(size=(2, 3))\n",
    "x2 = torch.rand(size=(2, 3))\n",
    "# x3 = x1 + x2 这种直接的方式就是将两者相加的结果赋予一个新的变量x3，而x1和x2的值并未改变\n",
    "# x1.add(x2) 这种方式也是一样，x1和x2的值并未改变，相加结果返回\n",
    "x1.add_(x2) #这种方式会将相加的结果赋予x1，使得x1的值改变,这种加上下划线的函数代表就地改变这个变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1.reshape(shape=(3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.9255, 1.3272],\n",
       "        [1.1276, 1.1031],\n",
       "        [0.7325, 0.4548]])"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "x1.view(size=(3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1.reshape(shape=(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.9255],\n",
       "        [1.3272],\n",
       "        [1.1276],\n",
       "        [1.1031],\n",
       "        [0.7325],\n",
       "        [0.4548]])"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "x1.view(size=(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view与reshape的区别\n",
    "# 1. https://blog.csdn.net/zplai/article/details/111416053\n",
    "# 2. https://blog.csdn.net/weixin_43002433/article/details/104299896?utm_medium=distribute.pc_relevant.none-task-blog-baidujs_title-0&spm=1001.2101.3001.4242"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.9451)"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "# x1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(6.2387)"
      ]
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "x1 = x1.sum()\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6.238728046417236"
      ]
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "source": [
    "x1.item()"
   ]
  },
  {
   "source": [
    "## 张量的自动微分\n",
    ">将torch.Tensor属性中，如果将requires_grad设置为True，pytorch将开始跟踪对此张量的所有操作，完成计算后，可以调用.backward()并自动计算所有的梯度，该张量的梯度将累加到.grad属性中"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 125
    }
   ],
   "source": [
    "# torch中一个Tensor张量数据包含三部分属性：\n",
    "# 1. data属性，也即记录这个张量的数据具体是多少；\n",
    "# 2. grad属性，代表张量的梯度，如果requires_grad=true的时候，则.backward()计算得到的梯度值就会被记录在这个张量的grad这个属性中；\n",
    "# 3. grad_fn属性，用于记录被计算出来的grad的值是由什么func计算得来的\n",
    "x = torch.ones(size=(2, 2), requires_grad=True)\n",
    "x.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 126
    }
   ],
   "source": [
    "x.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[3., 3.],\n",
       "        [3., 3.]], grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 129
    }
   ],
   "source": [
    "y = x + 2 # 广播机制\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x19aec49ed00>"
      ]
     },
     "metadata": {},
     "execution_count": 130
    }
   ],
   "source": [
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y*y + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(12., grad_fn=<MeanBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 132
    }
   ],
   "source": [
    "out = z.mean()\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1.5000, 1.5000],\n",
       "        [1.5000, 1.5000]])"
      ]
     },
     "metadata": {},
     "execution_count": 133
    }
   ],
   "source": [
    "out.backward() # d out / dx，这一步是求微分的操作\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 134
    }
   ],
   "source": [
    "x.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print((x**2).requires_grad)\n",
    "# with torch.no_grad() <=====> x.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([2., 3.], requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 138
    }
   ],
   "source": [
    "a = torch.tensor([2, 3], dtype=torch.float32)\r\n",
    "a.requires_grad_(True) #只有float类型的tensor才能使用requires_grad_"
   ]
  }
 ]
}